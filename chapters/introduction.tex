Extractive summarization remains a simple 
and fast approach to produce summaries which 
are grammatical and accurately represent the source text.
In the news domain, these systems are able to 
use a dominant signal: the position of a sentence 
in the source document. 
Due to journalistic conventions which place important information
early in the articles, the lead sentences often contain key information. In this paper, we explore how systems can look beyond this simple trend. 

Naturally, automatic systems have 
all along exploited position cues in news 
as key indicators of important content \parencite{schiffman,hong2014improving,ext_bert}. 
The `lead' baseline is rather strong in single-document news summarization \parencite{brandow1995automatic,nenkova2005automatic}, 
with automatic systems only modestly improving the results. 
Nevertheless, more than 20-30\% of 
summary-worthy sentences come from the second half of news documents \parencite{data2_nallapati2016abstractive,kedzie2018content}, 
and the lead baseline, as shown in Table \ref{tab:lead_ex},
does not always produce convincing summaries.
So, systems must balance the position bias with representations of the semantic content 
throughout the document. Alas, preliminary studies
\parencite{kedzie2018content} suggest that even the most recent neural 
methods predominantly pick sentences from the lead, and 
that their content selection performance drops greatly
when the position cues are withheld.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|p{0.94\linewidth}|}
        \hline
        \textbf{Lead-3:} Bangladesh beat fellow World Cup quarter-finalists Pakistan by 79 runs in the first one-day international in Dhaka. Tamim Iqbal and Mushfiqur Rahim scored centuries as Bangladesh made 329 for six and Pakistan could only muster 250 in reply. Pakistan will have the chance to level the three-match series on Sunday when the second odi takes place in Mirpur. \\ \hline
        \textbf{Reference:} Bangladesh beat fellow World Cup quarter-finalists Pakistan by 79 runs. Tamim Iqbal and Mushfiqur Rahim scored centuries for Bangladesh. Bangladesh made 329 for six and Pakistan could only muster 250 in reply. Pakistan will have the chance to level the three-match series on Sunday. \\ \hline \hline
        \textbf{Lead-3}: Standing up for what you believe. What does it cost you? What do you gain? \\ \hline
        \textbf{Reference:} Indiana town's Memories Pizza is shut down after online threat. Its owners say they'd refuse to cater a same-sex couple's wedding. \\
        \hline
    \end{tabular}
    \caption{`Lead' (first 3 sentences of source) can produce extremely faithful (top) to disastrously inaccurate (bottom) summaries. Gold standard summaries are also shown.}
    \label{tab:lead_ex}
\end{table}

In this paper, we verify that 
sentence position and lead bias dominate the 
learning signal for  state-of-the-art neural 
extractive summarizers in the news domain. 
We then present techniques to improve content selection in 
the face of this bias. 
The first technique makes use of `unbiased data' 
created by permuting the order of sentences in the training
articles. We use this shuffled dataset for pre-training, followed by 
training on the original (unshuffled) articles. 
The second method introduces an auxiliary loss 
which encourages the model's scores for sentences 
to mimic an estimated score distribution over the
sentences, the latter computed using ROUGE overlap 
with the gold standard. We implement these techniques 
for two recent reinforcement learning based systems, 
RNES \parencite{DBLP:conf/aaai/WuH18} and 
BanditSum \parencite{dong2018banditsum}, and evaluate 
them on the CNN/Daily Mail dataset \parencite{hermann2015teaching}.

We find that our auxiliary loss achieves 
significantly better ROUGE scores
compared to the base systems, and that the 
improvement is even more pronounced when the true 
best sentences appear later in the article. 
On the other hand, the pretraining approach produces mixed results.
We also confirm that when summary-worthy sentences 
appear late, there is a large performance discrepancy 
between the oracle summary and state-of-the-art summarizers,
indicating that learning to balance lead bias with 
other features of news text is a noteworthy issue to tackle.

\subsection{Outline}

Basic theory of stochastic processes and Markov chains is first presented in chapter \ref{chap:decisionmaking}. The inclusion of this material is motivated by the probabilistic interpretation of spectral graph theory studied in chapter \ref{chap:dynamics}. The theory of Markov Decision Processes is presented in section \ref{sec:mdp} as a necessary prerequisite for the appreciation of the techniques developed in reinforcement learning. Chapter \ref{chap:temporalabstraction} focuses on the presentation of the options framework \parencite{Sutton1999} in reinforcement learning as a way to represent temporal abstraction and learn optimal control over it.

The connection from graph \textit{structure} to system \textit{dynamics} is developed throughout chapter \ref{chap:dynamics} and is instrumental is understanding the strengths and pitfalls of the graph partitioning approach for options discovery. It also allows a better understanding of the relevant work on Nearly-Completely Decomposable Markov Chains (NCD) for future theoretical research on the bottleneck concept. The NCD theory seems to call for an information theoretic comprehension of temporal abstraction which is briefly developed at the end of this section.

A new algorithm for options discovery is proposed in chapter \ref{chap:buildingoptions} based on the  \textsc{Walktrap} community detection algorithm of \cite{Pons2005}. Although \textsc{Walktrap} finds its roots into spectral graph theory, its running time is only order $\mathcal{O}(mn^2)$ rather than $\mathcal{O}(n^3)$ by avoiding to compute the eigenvectors explicitly. The problem of options discovery and construction is also set under the assumption of a continuous state space. Techniques for constructing proximity graphs in Euclidean space are developed in section \ref{sec:proximitygraphs}. Section \ref{sec:knnoptions} shows how approximate nearest neighbors algorithms can be used to properly define the initiation and termination components of options under continuous observations.

An illustration of the proposed algorithm is provided in chapter \ref{chap:illustration} with the Pinball domain of \cite{Konidaris2009}. Practical difficulties having to do oscillations and off-policy learning are analysed. The proper empirical choices for the number of nearest neighbors, type of proximity graph and time scale for the \textsc{Walktrap} algorithm are discussed.